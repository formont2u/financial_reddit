{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows to scrap data from different subreddit, and keep the dataset up to date. It can be used in any subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs imported\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "print('Libs imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_from_utc(subreddit,created_utc):\n",
    "    url_sub = 'https://api.pushshift.io/reddit/search/submission/?size=1000&after='+str(created_utc)+'&subreddit='+str(subreddit)\n",
    "    r_sub = requests.get(url_sub)\n",
    "    data_sub = json.loads(r_sub.text)\n",
    "    return data_sub\n",
    "\n",
    "def get_submission_before_utc(subreddit,created_utc):\n",
    "    url_sub = 'https://api.pushshift.io/reddit/search/submission/?size=1000&before='+str(created_utc)+'&subreddit='+str(subreddit)\n",
    "    r_sub = requests.get(url_sub)\n",
    "    data_sub = json.loads(r_sub.text)\n",
    "    return data_sub\n",
    "\n",
    "def create_internal_dic(submission):\n",
    "    error_count=0\n",
    "    com_list=[]\n",
    "    try:\n",
    "        if '[removed]'in submission['selftext']:\n",
    "            internal_dict=0\n",
    "        else:\n",
    "            try:\n",
    "                sub_id=submission['id']\n",
    "                url_com='https://api.pushshift.io/reddit/comment/search/?link_id='+str(sub_id)+'&limit=20000'\n",
    "                data_com=json.loads(requests.get(url_com).text)\n",
    "                for com in data_com['data']:\n",
    "                    com_list.append(com['body'])\n",
    "                internal_dict={'id':submission['id'],'created_utc':submission['created_utc'],'body':submission['selftext'],'commentaries':com_list}\n",
    "            except:\n",
    "                error_count=error_count+1\n",
    "                internal_dict=0\n",
    "    except:\n",
    "        internal_dict=0\n",
    "        error_count=error_count+1\n",
    "    return {'dic':internal_dict, 'errors':error_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the subreddit to get the data from:\n",
      "investing\n",
      "If you want to scrap another subreddit you can write its name. Else press enter\n",
      "\n",
      "How many days of data do you want to gather for these subreddits2\n",
      "Data gathering for investing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"sub_scrapping.json\",\"r\") as data:\n",
    "        main_json=json.load(data)\n",
    "        data.close()\n",
    "        print('File already exist')\n",
    "\n",
    "except:\n",
    "    \n",
    "    with open(\"sub_scrapping.json\",\"w\") as data:\n",
    "        subreddit_list=[]\n",
    "        print('Enter the name of the subreddit to get the data from:')\n",
    "        subreddit=input()\n",
    "\n",
    "        while subreddit!='':\n",
    "            print('If you want to scrap another subreddit you can write its name. Else press enter')\n",
    "            subreddit_list.append(subreddit)\n",
    "            subreddit=input()\n",
    "\n",
    "        day_stamp=int(input('How many days of data do you want to gather for these subreddits'))\n",
    "\n",
    "        data.write('{\"data\":{')\n",
    "        for subreddit in subreddit_list:\n",
    "            ini=False\n",
    "            count=0\n",
    "            total_errors=0\n",
    "            ini_number=0\n",
    "            print('Data gathering for '+str(subreddit))\n",
    "            data.write('\"'+str(subreddit)+'\":[')\n",
    "            today_date = datetime.datetime.today()\n",
    "            start_date = today_date - datetime.timedelta(days=day_stamp)\n",
    "            scrap_date=int(start_date.timestamp())\n",
    "            data_sub=get_submission_from_utc(subreddit,scrap_date)\n",
    "            while not ini:\n",
    "                ini_number=ini_number+1\n",
    "                dic=create_internal_dic(data_sub['data'][ini_number-1])\n",
    "                if dic['dic']==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    data.write(json.dumps(dic['dic']))\n",
    "                    ini=True\n",
    "            for sub in data_sub['data'][ini:]:\n",
    "                dic=create_internal_dic(sub)\n",
    "                if dic['dic']==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    data.write(','+json.dumps(dic['dic']))\n",
    "                total_errors=total_errors+dic['errors']\n",
    "\n",
    "            data_sub=get_submission_from_utc(subreddit,data_sub['data'][len(data_sub['data'])-1]['created_utc'])    \n",
    "            while len(data_sub['data'])!=0:\n",
    "                count=count+1\n",
    "\n",
    "                for sub in data_sub['data']:\n",
    "                    dic=create_internal_dic(sub)\n",
    "                    if dic['dic']==0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        data.write(','+json.dumps(dic['dic']))\n",
    "                    total_errors=total_errors+dic['errors']            \n",
    "\n",
    "                print('\\t'+str(count)+' Data scrapped to the '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(data_sub['data'][len(data_sub['data'])-1]['created_utc']))+' with a total of '+str(total_errors)+' errors')\n",
    "                data_sub=get_submission_from_utc(subreddit,data_sub['data'][len(data_sub['data'])-1]['created_utc'])\n",
    "            if subreddit_list.index(subreddit)==len(subreddit_list)-1:\n",
    "                data.write(']}')\n",
    "            else:\n",
    "                data.write('],')\n",
    "        data.write('\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In few words, the code above creates a json file containing every submission and commentaries from one or more subreddit. It serves as a database initialization.\n",
    "\n",
    "\n",
    "The structure of the JSON will be :\n",
    "{\n",
    "    data:\n",
    "         {subreddit1:\n",
    "                    [{'id':str,'created_utc':int,'body':str,'commentaries':list]\n",
    "         ,\n",
    "         subreddit2:\n",
    "                    [{}]\n",
    "         }\n",
    "}\n",
    "                        \n",
    "\n",
    "Let's breakdown the code piece by piece. The initialization is like that: \n",
    "1. \n",
    "\n",
    "```python\n",
    "try:\n",
    "    with open(\"sub_scrapping.json\",\"r\") as data:\n",
    "        main_json=json.load(data)\n",
    "        data.close()\n",
    "        print('File already exist')\n",
    "\n",
    "except:\n",
    "    \n",
    "    with open(\"sub_scrapping.json\",\"w\") as data:\n",
    "        subreddit_list=[]\n",
    "        print('Enter the name of the subreddit to get the data from:')\n",
    "        subreddit=input()\n",
    "\n",
    "        while subreddit!='':\n",
    "            print('If you want to scrap another subreddit you can write its name. Else press enter')\n",
    "            subreddit_list.append(subreddit)\n",
    "            subreddit=input()\n",
    "\n",
    "        day_stamp=int(input('How many days of data do you want to gather for these subreddits'))\n",
    "\n",
    "        data.write('{\\n\\t\"data\":')\n",
    "```\n",
    "This part create a file that will be then used as a data base. As it creates a file, it can delete it, so it is important to check first if the file exists, this is the role of the try except. If the JSON file is loaded without a problem, the script ends.\n",
    "\n",
    "Otherwise, the user has to input 2 informations:\n",
    "- the subreddits he wants to scrap\n",
    "- the number of days he wants to scrap\n",
    "\n",
    "As it can be a very time consuming script, it is better to initialize with a small amount of days, and then use the next part of the program, which is dedicated to modify the database\n",
    "\n",
    "2.\n",
    "\n",
    "Next, the script will iterate through the list of subreddit the user wrote: \n",
    "\n",
    "```python\n",
    "        for subreddit in subreddit_list:\n",
    "            ini=False\n",
    "            count=0\n",
    "            total_errors=0\n",
    "            ini_number=0\n",
    "            print('Data gathering for '+str(subreddit))\n",
    "            data.write('\\n\\t\\t{\\n\\t\\t\\t\"'+str(subreddit)+'\": \\n\\t\\t\\t\\t[')\n",
    "            today_date = datetime.datetime.today()\n",
    "            start_date = today_date - datetime.timedelta(days=day_stamp)\n",
    "            scrap_date=int(start_date.timestamp())\n",
    "            \n",
    "```\n",
    "Before anything, it is important to know that the way data are written in the file is a bit different than expected. What could be expected, is that once the dictionnary is built the data is then written in the file. However I had a lot of internet issues, and needed a way to keep the data even if the programm crashed due to internet issues.\n",
    "Therefore, each time the programm iterates, it writes data in the file, not in the end. To make it simple, instead of writing the entire file at once, it writes it line by line.\n",
    "\n",
    "ini=False is to make a special case of the first \"submission\" treated. Because data is getting written item by item, a coma is added before one, so when it is loaded as a JSON file there are major issues.\n",
    "\n",
    "count=0 is to show the number of iteration each command needs. It is reseted for each subreddit.\n",
    "total_errors is self explanatory, it keep tracks of the number of time the algorithm didn't manage to scrap the submissions.\n",
    "\n",
    "For the file creation, the logic is the following:\n",
    "The user wants to scrap x days, so the program substracts x days to today and the script scrap the data from the scrap date (epoch format)\n",
    "\n",
    "3.\n",
    "\n",
    "Then the data gathering happens.\n",
    "\n",
    "```python\n",
    "data_sub=get_submission_from_utc(subreddit,scrap_date)\n",
    "            while not ini:\n",
    "                ini_number=ini_number+1\n",
    "                dic=create_internal_dic(data_sub['data'][ini_number-1])\n",
    "                if dic['dic']==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    data.write(json.dumps(dic['dic']))\n",
    "                    ini=True\n",
    "            for sub in data_sub['data'][ini:]:\n",
    "                dic=create_internal_dic(sub)\n",
    "                if dic['dic']==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    data.write(','+json.dumps(dic['dic']))\n",
    "                total_errors=total_errors+dic['errors']\n",
    "```\n",
    "\n",
    "The function: get_function_from_utc() is an api calls to pushift.io, a usefull tool to scrap reddit, the function returns 100 submissions maximum.\n",
    "\n",
    "Then, the create_internal_dic() is here to create a dictionnary including the id, created_utc, body and commentaries of one submission. It returns the dic plus the number of errors encountered.\n",
    "\n",
    "There is a specific condition for the very first item, as the dic is dumped without coma, then the entire data gathering takes place.\n",
    "\n",
    "The one important thing to notice is that the scrap date is replaced every iteration by the most recent dates. \n",
    "The iteration process stops when no more submissions are returned ie. when the length of the list equals 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second piece of code is to manage the data file:\n",
    "There are 3 different actions possible:\n",
    "- Updating the database (UP) will get the most recent date (highest epoch) and scrap every subreddits already in the file until today's date\n",
    "- Get more data will allow the user to collect more data, anterior to the oldest date (lowest epoch) in the dataset. \n",
    "- Add a subreddit, will append another subreddit to the already existing subreddit list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits already scrapped: ['pennystocks', 'investing', 'wallstreetbets']\n",
      "Update the list, Add another subreddit or Get more data (UP/ADD/GET)?get\n",
      "How many days do you want to add ?2\n",
      "Nombre d'éléments avant addition des données dans pennystocks: 954\n",
      "Last date detected for pennystocks: 2021-03-12 02:26:01\n",
      "Data updating\n",
      "\t1 Data updated to the 2021-03-11 21:30:39 with a total of 0 errors\n",
      "\t2 Data updated to the 2021-03-11 18:48:25 with a total of 0 errors\n",
      "\t3 Data updated to the 2021-03-11 15:51:23 with a total of 0 errors\n",
      "\t4 Data updated to the 2021-03-11 05:45:02 with a total of 0 errors\n",
      "\t5 Data updated to the 2021-03-10 21:52:50 with a total of 0 errors\n",
      "\t6 Data updated to the 2021-03-10 17:54:53 with a total of 0 errors\n",
      "\t7 Data updated to the 2021-03-10 14:01:39 with a total of 0 errors\n",
      "\t8 Data updated to the 2021-03-10 02:34:09 with a total of 0 errors\n",
      "\t9 Data updated to the 2021-03-09 20:36:55 with a total of 0 errors\n",
      "Nombre d'éléments après addition des données pennystocks: 1460\n",
      "Nombre d'éléments avant addition des données dans investing: 143\n",
      "Last date detected for investing: 2021-03-12 14:43:01\n",
      "Data updating\n",
      "\t1 Data updated to the 2021-03-12 01:29:21 with a total of 0 errors\n",
      "\t2 Data updated to the 2021-03-11 18:57:44 with a total of 0 errors\n",
      "\t3 Data updated to the 2021-03-11 14:32:03 with a total of 0 errors\n",
      "\t4 Data updated to the 2021-03-11 00:57:47 with a total of 0 errors\n",
      "\t5 Data updated to the 2021-03-10 19:18:02 with a total of 0 errors\n",
      "\t6 Data updated to the 2021-03-10 14:22:28 with a total of 0 errors\n",
      "Nombre d'éléments après addition des données investing: 224\n",
      "Nombre d'éléments avant addition des données dans wallstreetbets: 18005\n",
      "Last date detected for wallstreetbets: 2021-03-12 10:47:19\n",
      "Data updating\n",
      "\t1 Data updated to the 2021-03-12 09:39:37 with a total of 0 errors\n",
      "\t2 Data updated to the 2021-03-12 08:49:32 with a total of 0 errors\n",
      "\t3 Data updated to the 2021-03-12 07:58:18 with a total of 0 errors\n",
      "\t4 Data updated to the 2021-03-12 07:09:37 with a total of 0 errors\n",
      "\t5 Data updated to the 2021-03-12 06:26:44 with a total of 0 errors\n",
      "\t6 Data updated to the 2021-03-12 05:52:42 with a total of 0 errors\n",
      "\t7 Data updated to the 2021-03-12 05:19:09 with a total of 0 errors\n",
      "\t8 Data updated to the 2021-03-12 04:50:27 with a total of 0 errors\n",
      "\t9 Data updated to the 2021-03-12 04:13:04 with a total of 0 errors\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "total_errors=0\n",
    "with open(\"sub_scrapping.json\",\"r\") as data:\n",
    "    main_json=json.load(data)\n",
    "    data.close()\n",
    "existing_sub=list(main_json['data'].keys())\n",
    "action=input('Subreddits already scrapped: '+str(existing_sub)+'\\nUpdate the list, Add another subreddit or Get more data (UP/ADD/GET)?')\n",
    "if str(action).upper() =='UP':\n",
    "    for subreddit in existing_sub:\n",
    "        print(\"Number of submissions before update in \"+str(subreddit)+\": \"+str(len(main_json['data'][subreddit])))\n",
    "        count=0\n",
    "        max_date=main_json['data'][subreddit][0]['created_utc']\n",
    "        for submission in main_json['data'][subreddit]:\n",
    "            if submission['created_utc']>max_date:\n",
    "                max_date=submission['created_utc']\n",
    "            else:\n",
    "                pass\n",
    "        print('Last date detected for '+str(subreddit)+': '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(max_date)))\n",
    "        print('Data updating')\n",
    "        data_sub=get_submission_from_utc(subreddit,max_date)\n",
    "        while len(data_sub['data'])!=0:\n",
    "            count=count+1\n",
    "            for sub in data_sub['data']:\n",
    "                dic=create_internal_dic(sub)\n",
    "                if dic['dic']==0:\n",
    "                    print(sub['selftext'])\n",
    "                else:\n",
    "                    main_json['data'][subreddit].append(dic['dic'])\n",
    "            max_date=data_sub['data'][len(data_sub['data'])-1]['created_utc']\n",
    "            total_errors=total_errors+dic['errors']  \n",
    "            data_sub=get_submission_from_utc(subreddit,max_date)\n",
    "            print('\\t'+str(count)+' Data updated to the '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(max_date))+' with a total of '+str(total_errors)+' errors')\n",
    "        print(\"Number of submissions after update in \"+str(subreddit)+\": \"+str(len(main_json['data'][subreddit])))\n",
    "\n",
    "elif str(action).upper()=='GET':\n",
    "    date_to_add=input('How many days do you want to add ?')\n",
    "    for subreddit in existing_sub:\n",
    "        print(\"Number of submissions before data gathering in \"+str(subreddit)+\": \"+str(len(main_json['data'][subreddit])))\n",
    "        count=0\n",
    "        min_date=main_json['data'][subreddit][0]['created_utc']\n",
    "        for submission in main_json['data'][subreddit]:\n",
    "            if submission['created_utc']<min_date:\n",
    "                min_date=submission['created_utc']\n",
    "            else:\n",
    "                pass\n",
    "        print('Last date detected for '+str(subreddit)+': '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(min_date)))\n",
    "        print('Data updating')\n",
    "        goal_date = datetime.datetime.fromtimestamp(min_date) - datetime.timedelta(days=int(date_to_add))\n",
    "        goal_epoch=int(goal_date.timestamp())\n",
    "        while min_date>goal_epoch:\n",
    "            data_sub=get_submission_before_utc(subreddit,min_date)\n",
    "            count=count+1\n",
    "            for sub in data_sub['data']:\n",
    "                dic=create_internal_dic(sub)\n",
    "                if dic['dic']==0:\n",
    "                    pass\n",
    "                else:\n",
    "                    main_json['data'][subreddit].append(dic['dic'])\n",
    "            min_date=data_sub['data'][len(data_sub['data'])-1]['created_utc']\n",
    "            total_errors=total_errors+dic['errors']  \n",
    "            print('\\t'+str(count)+' Data updated to the '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(min_date))+' with a total of '+str(total_errors)+' errors')\n",
    "        print(\"Number of submissions after data gathering in  \"+str(subreddit)+\": \"+str(len(main_json['data'][subreddit])))\n",
    "\n",
    "elif str(action).upper()=='ADD':\n",
    "    sub_to_add=input('Which subreddit do you want to add ?')\n",
    "    limit_date=input('How many days do you want to scrap ?')\n",
    "    goal_date = datetime.datetime.today() - datetime.timedelta(days=int(limit_date))\n",
    "    goal_epoch=int(goal_date.timestamp())\n",
    "    scrap_date=int(datetime.datetime.today().timestamp())\n",
    "    main_json['data'][sub_to_add]=[]\n",
    "    while scrap_date>goal_epoch:\n",
    "        data_sub=get_submission_before_utc(sub_to_add,scrap_date)\n",
    "        count=count+1\n",
    "        for sub in data_sub['data']:\n",
    "            dic=create_internal_dic(sub)\n",
    "            if dic['dic']==0:\n",
    "                pass\n",
    "            else:\n",
    "                 main_json['data'][sub_to_add].append(dic['dic'])\n",
    "        scrap_date=data_sub['data'][len(data_sub['data'])-1]['created_utc']\n",
    "        total_errors=total_errors+dic['errors']  \n",
    "        print('\\t'+str(count)+' Data added to the '+time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(scrap_date))+' with a total of '+str(total_errors)+' errors')\n",
    "    print(\"Number of submissions written in \"+str(sub_to_add)+\": \"+str(len(main_json['data'][sub_to_add])))\n",
    "    \n",
    "    \n",
    "with open(\"sub_scrapping.json\",\"w\") as data:\n",
    "    json_file=json.dumps(main_json)\n",
    "    data.write(json_file)\n",
    "    data.close()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
